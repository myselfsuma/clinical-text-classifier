{"cells":[{"cell_type":"markdown","metadata":{},"source":["Here we look at Medical Transcriptions dataset from Kaggle\n","https://www.kaggle.com/tboyle10/medicaltranscriptions\n","\n","This data was scraped from mtsamples.com\n","\n","Inspiration\n","Can you correctly classify the medical specialties based on the transcription text?\n"]},{"cell_type":"markdown","metadata":{},"source":["Let us import all the necessary libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'nltk'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer \n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"]}],"source":["# import package\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import string\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.manifold import TSNE\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","from nltk.stem import WordNetLemmatizer \n","\n","from imblearn.over_sampling import SMOTE\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('changes doing')"]},{"cell_type":"markdown","metadata":{},"source":["A method to get unique words(vocabulary) and sentence count in a list of text "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_sentence_word_count(text_list):\n","    sent_count = 0\n","    word_count = 0\n","    vocab = {}\n","    for text in text_list:\n","        sentences=sent_tokenize(str(text).lower())\n","        sent_count = sent_count + len(sentences)\n","        for sentence in sentences:\n","            words=word_tokenize(sentence)\n","            for word in words:\n","                if(word in vocab.keys()):\n","                    vocab[word] = vocab[word] +1\n","                else:\n","                    vocab[word] =1 \n","    word_count = len(vocab.keys())\n","    return sent_count,word_count\n","    "]},{"cell_type":"markdown","metadata":{},"source":["Lets do some exploratory analysis of data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clinical_text_df = pd.read_csv(\"/kaggle/input/medicaltranscriptions/mtsamples.csv\")\n","\n","print(clinical_text_df.columns)\n","clinical_text_df.head(5)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","clinical_text_df = clinical_text_df[clinical_text_df['transcription'].notna()]\n","sent_count,word_count= get_sentence_word_count(clinical_text_df['transcription'].tolist())\n","print(\"Number of sentences in transcriptions column: \"+ str(sent_count))\n","print(\"Number of unique words in transcriptions column: \"+str(word_count))\n","\n","\n","\n","data_categories  = clinical_text_df.groupby(clinical_text_df['medical_specialty'])\n","i = 1\n","print('===========Original Categories =======================')\n","for catName,dataCategory in data_categories:\n","    print('Cat:'+str(i)+' '+catName + ' : '+ str(len(dataCategory)) )\n","    i = i+1\n","print('==================================')\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Since some catgeories have less than 50 samples i remove them"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["filtered_data_categories = data_categories.filter(lambda x:x.shape[0] > 50)\n","final_data_categories = filtered_data_categories.groupby(filtered_data_categories['medical_specialty'])\n","i=1\n","print('============Reduced Categories ======================')\n","for catName,dataCategory in final_data_categories:\n","    print('Cat:'+str(i)+' '+catName + ' : '+ str(len(dataCategory)) )\n","    i = i+1\n","\n","print('============ Reduced Categories ======================')\n"]},{"cell_type":"markdown","metadata":{},"source":["Lets plot the categories"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,10))\n","sns.countplot(y='medical_specialty', data = filtered_data_categories )\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We are interested only in the 'transcription' and 'medical_specialty' columns in the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = filtered_data_categories[['transcription', 'medical_specialty']]\n","data = data.drop(data[data['transcription'].isna()].index)\n","data.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('Sample Transcription 1:'+data.iloc[5]['transcription']+'\\n')\n","print('Sample Transcription 2:'+data.iloc[125]['transcription']+'\\n')\n","print('Sample Transcription 3:'+data.iloc[1000]['transcription'])\n"]},{"cell_type":"markdown","metadata":{},"source":["Lets define soome methods for cleaning the data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def clean_text(text ): \n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    text1 = ''.join([w for w in text if not w.isdigit()]) \n","    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n","    #BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n","    \n","    text2 = text1.lower()\n","    text2 = REPLACE_BY_SPACE_RE.sub('', text2) # replace REPLACE_BY_SPACE_RE symbols by space in text\n","    #text2 = BAD_SYMBOLS_RE.sub('', text2)\n","    return text2\n","\n","def lemmatize_text(text):\n","    wordlist=[]\n","    lemmatizer = WordNetLemmatizer() \n","    sentences=sent_tokenize(text)\n","    \n","    intial_sentences= sentences[0:1]\n","    final_sentences = sentences[len(sentences)-2: len(sentences)-1]\n","    \n","    for sentence in intial_sentences:\n","        words=word_tokenize(sentence)\n","        for word in words:\n","            wordlist.append(lemmatizer.lemmatize(word))\n","    for sentence in final_sentences:\n","        words=word_tokenize(sentence)\n","        for word in words:\n","            wordlist.append(lemmatizer.lemmatize(word))       \n","    return ' '.join(wordlist) \n"]},{"cell_type":"markdown","metadata":{},"source":["Lets clean the data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","data['transcription'] = data['transcription'].apply(lemmatize_text)\n","data['transcription'] = data['transcription'].apply(clean_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('Sample Transcription 1:'+data.iloc[5]['transcription']+'\\n')\n","print('Sample Transcription 2:'+data.iloc[125]['transcription']+'\\n')\n","print('Sample Transcription 3:'+data.iloc[1000]['transcription'])"]},{"cell_type":"markdown","metadata":{},"source":["Lets us peform feature extraction using TfidfVectorizer to generate tf-idf features.\n","For more on tf-idf check here: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n","In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75, use_idf=True, smooth_idf=True, max_features=1000)\n","tfIdfMat  = vectorizer.fit_transform(data['transcription'].tolist() )\n","feature_names = sorted(vectorizer.get_feature_names())\n","print(feature_names)"]},{"cell_type":"markdown","metadata":{},"source":["Lets visualize the tf-idf features using t-sne plot. For more on t-sne check here: https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n","T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton.[1] It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n","The t-sne plot shows that lot of categories are overlapping with each other."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()\n","tfIdfMatrix = tfIdfMat.todense()\n","labels = data['medical_specialty'].tolist()\n","tsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(tfIdfMatrix)\n","plt.figure(figsize=(16,10))\n","palette = sns.hls_palette(21, l=.6, s=.9)\n","sns.scatterplot(\n","    x=tsne_results[:,0], y=tsne_results[:,1],\n","    hue=labels,\n","    palette= palette,\n","    legend=\"full\",\n","    alpha=0.3\n",")\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Let us do PCA to reduce dimensionality of features.\n","https://en.wikipedia.org/wiki/Principal_component_analysis\n","PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gc.collect()\n","pca = PCA(n_components=0.95)\n","tfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())\n","labels = data['medical_specialty'].tolist()\n","category_list = data.medical_specialty.unique()\n","X_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels,random_state=1)   \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('Train_Set_Size:'+str(X_train.shape))\n","print('Test_Set_Size:'+str(X_test.shape))"]},{"cell_type":"markdown","metadata":{},"source":["Let us use Logisitic Regression to learn on training data and predict on test data\n","https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1).fit(X_train, y_train)\n","y_test_pred= clf.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["Let us visualize the confusion matrix and the classification results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["labels = category_list\n","cm = confusion_matrix(y_test, y_test_pred, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","fig = plt.figure(figsize=(20,20))\n","ax= fig.add_subplot(1,1,1)\n","sns.heatmap(cm, annot=True, cmap=\"Greens\",ax = ax,fmt='g'); #annot=True to annotate cells\n","\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n","ax.set_title('Confusion Matrix'); \n","ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n","plt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\n","plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')     \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(classification_report(y_test,y_test_pred,labels=category_list))"]},{"cell_type":"markdown","metadata":{},"source":["The results are quite poor. Let us apply some domain knowledge and see if we can improve the results\n","The surgey category is kind of superset as there can be surgeries belonging to specializations like cardiology,neurolrogy etc. Similarly other categories like Emergency Room Reports, Discharge Summary, Notes also overlap with specialities. Hence i remove them."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["filtered_data_categories['medical_specialty'] =filtered_data_categories['medical_specialty'].apply(lambda x:str.strip(x))\n","mask = filtered_data_categories['medical_specialty'] == 'Surgery'\n","filtered_data_categories = filtered_data_categories[~mask]\n","final_data_categories = filtered_data_categories.groupby(filtered_data_categories['medical_specialty'])\n","mask = filtered_data_categories['medical_specialty'] == 'SOAP / Chart / Progress Notes'\n","filtered_data_categories = filtered_data_categories[~mask]\n","mask = filtered_data_categories['medical_specialty'] == 'Office Notes'\n","filtered_data_categories = filtered_data_categories[~mask]\n","mask = filtered_data_categories['medical_specialty'] == 'Consult - History and Phy.'\n","filtered_data_categories = filtered_data_categories[~mask]\n","mask = filtered_data_categories['medical_specialty'] == 'Emergency Room Reports'\n","filtered_data_categories = filtered_data_categories[~mask]\n","mask = filtered_data_categories['medical_specialty'] == 'Discharge Summary'\n","filtered_data_categories = filtered_data_categories[~mask]\n","\n","'''\n","mask = filtered_data_categories['medical_specialty'] == 'Pediatrics - Neonatal'\n","filtered_data_categories = filtered_data_categories[~mask]\n","'''\n","mask = filtered_data_categories['medical_specialty'] == 'Pain Management'\n","filtered_data_categories = filtered_data_categories[~mask]\n","mask = filtered_data_categories['medical_specialty'] == 'General Medicine'\n","filtered_data_categories = filtered_data_categories[~mask]\n","\n","\n","mask = filtered_data_categories['medical_specialty'] == 'Neurosurgery'\n","filtered_data_categories.loc[mask, 'medical_specialty'] = 'Neurology'\n","mask = filtered_data_categories['medical_specialty'] == 'Nephrology'\n","filtered_data_categories.loc[mask, 'medical_specialty'] = 'Urology'\n","\n","\n","i=1\n","print('============Reduced Categories======================')\n","for catName,dataCategory in final_data_categories:\n","    print('Cat:'+str(i)+' '+catName + ' : '+ str(len(dataCategory)) )\n","    i = i+1\n","\n","print('============Reduced Categories======================')\n","\n","\n","data = filtered_data_categories[['transcription', 'medical_specialty']]\n","data = data.drop(data[data['transcription'].isna()].index)\n","data.shape"]},{"cell_type":"markdown","metadata":{},"source":["Let us use sciscpacy models to detect medical entities in our text\n","scispaCy is a Python package containing spaCy models for processing biomedical, scientific or clinical text.\n","For more on scispacy check here:https://allenai.github.io/scispacy/"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_ner_bionlp13cg_md-0.2.5.tar.gz\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import spacy\n","import en_ner_bionlp13cg_md\n","nlp = en_ner_bionlp13cg_md.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def process_Text( text):\n","    wordlist=[]\n","    doc = nlp(text)\n","    for ent in doc.ents:\n","        wordlist.append(ent.text)\n","    return ' '.join(wordlist)     "]},{"cell_type":"markdown","metadata":{},"source":["Let us pre-process data using scispacy to detect medical entities in transcriptions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data['transcription'] = data['transcription'].apply(process_Text)\n","data['transcription'] = data['transcription'].apply(lemmatize_text)\n","data['transcription'] = data['transcription'].apply(clean_text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","print('Sample Transcription 1:'+data.iloc[5]['transcription']+'\\n')\n","print('Sample Transcription 2:'+data.iloc[125]['transcription']+'\\n')\n","print('Sample Transcription 3:'+data.iloc[1000]['transcription'])"]},{"cell_type":"markdown","metadata":{},"source":["Let us extract tf-idf features then perform dimensionality reduction on the features using t-sne and plot the t-sne features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75,min_df=5, use_idf=True, smooth_idf=True,sublinear_tf=True, max_features=1000)\n","tfIdfMat  = vectorizer.fit_transform(data['transcription'].tolist() )\n","feature_names = sorted(vectorizer.get_feature_names())\n","print(feature_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()\n","tfIdfMatrix = tfIdfMat.todense()\n","labels = data['medical_specialty'].tolist()\n","tsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(tfIdfMatrix)\n","plt.figure(figsize=(20,10))\n","palette = sns.hls_palette(12, l=.3, s=.9)\n","sns.scatterplot(\n","    x=tsne_results[:,0], y=tsne_results[:,1],\n","    hue=labels,\n","    palette= palette,\n","    legend=\"full\",\n","    alpha=0.3\n",")\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pca = PCA(n_components=0.95)\n","tfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())\n","labels = data['medical_specialty'].tolist()\n","category_list = data.medical_specialty.unique()"]},{"cell_type":"markdown","metadata":{},"source":["Let us create train and test sets.Let us use logistic regression for developing a classification model and then visualize the results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels,random_state=1)   \n","print('Train_Set_Size:'+str(X_train.shape))\n","print('Test_Set_Size:'+str(X_test.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n","clf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1).fit(X_train, y_train)\n","y_test_pred= clf.predict(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["labels = category_list\n","cm = confusion_matrix(y_test, y_test_pred, labels)\n","\n","fig = plt.figure(figsize=(20,20))\n","ax= fig.add_subplot(1,1,1)\n","sns.heatmap(cm, annot=True, cmap=\"Greens\",ax = ax,fmt='g'); #annot=True to annotate cells\n","\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n","ax.set_title('Confusion Matrix'); \n","ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n","plt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\n","plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')     \n","plt.show()\n","print(classification_report(y_test,y_test_pred,labels=category_list))\n"]},{"cell_type":"markdown","metadata":{},"source":["There is marked improvement in results. Since some classes are in minority we can use SMOTE(Synthetic Minority Over-sampling Technique\n",") to generate more sample form minority class to solve the data imbalance problem. For more on SMOTE check here:https://arxiv.org/pdf/1106.1813.pdf. Let us generate new dataset using SMOTE and then perform classification on them\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["smote_over_sample = SMOTE(sampling_strategy='minority')\n","labels = data['medical_specialty'].tolist()\n","X, y = smote_over_sample.fit_resample(tfIdfMat_reduced, labels)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=1)   \n","print('Train_Set_Size:'+str(X_train.shape))\n","print('Test_Set_Size:'+str(X_test.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1).fit(X_train, y_train)\n","y_test_pred= clf.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["Let us visualize the data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["labels = category_list\n","cm = confusion_matrix(y_test, y_test_pred, labels)\n","\n","fig = plt.figure(figsize=(20,20))\n","ax= fig.add_subplot(1,1,1)\n","sns.heatmap(cm, annot=True, cmap=\"Greens\",ax = ax,fmt='g'); #annot=True to annotate cells\n","\n","# labels, title and ticks\n","ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n","ax.set_title('Confusion Matrix'); \n","ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n","plt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\n","plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')     \n","plt.show()\n","print(classification_report(y_test,y_test_pred,labels=category_list))"]},{"cell_type":"markdown","metadata":{},"source":["Still some categories are not getting classofoed properly.Let us look at samples from these classes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mask = filtered_data_categories['medical_specialty'] == 'Radiology'\n","radiologyData = filtered_data_categories[mask]\n","print(radiologyData['transcription'].tolist()[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mask = clinical_text_df['medical_specialty'] ==  ' Pediatrics - Neonatal'\n","pediaData = clinical_text_df[mask]\n","print(pediaData ['transcription'].tolist()[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","mask = clinical_text_df['medical_specialty'] ==  ' Hematology - Oncology'\n","oncoData = clinical_text_df[mask]\n","print(oncoData ['transcription'].tolist()[1])"]},{"cell_type":"markdown","metadata":{},"source":["My learnings from this dataset are:\n","This dataset is very noisy.\n","\n","Lot of text in transcriptions overlaps across categories\n","\n","We can apply domain knowledge to reduce the categories\n","\n","It is imbalanced dataset and using SMOTE can improve the results\n","\n","Hand coded features may improve results on this dataset but may not apply to generic transcription datasets."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":64826,"sourceId":127612,"sourceType":"datasetVersion"}],"dockerImageVersionId":29980,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
